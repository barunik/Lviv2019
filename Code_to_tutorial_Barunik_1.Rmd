---
title: 'Applied Econometrics (Part 1)'
subtitle: 'Lviv Data Science Summer School' 
author: "Jozef Barunik"
date: "July 2019"
output:
  html_document:
    fig.retina: 1
    fig_height: 3.5
    fig_width: 5
    keep_md: yes
    toc: true
    toc_float: true
    number_sections: true
  pdf_document:
    fig_height: 2.1
    fig_width: 4.2
    keep_tex: yes
    latex_engine: xelatex
---
<!-- #### Outline -->
<!-- * Linear AR, MA processes -->
<!-- * VAR analysis -->
<!-- * Connection to Machine Learning techniques -->
<!-- * Macro + Finance application -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#fig.width = 9.5, fig.height = 3.7
```

Let's install required packages first, please change the path

```{r results='hide', message=FALSE, warning=FALSE}

rm(list = ls())

library(tseries)
library(forecast)
library(vars)
library(mAr)

setwd("~/Documents/PhD/Konference/2018/Lviv_summer_school/Lecture notes/")
```


# Introduction to Time Series Econometrics

Let's consider a time series $x_t$ evolving over $t=1,...,T$

## Stationarity

A time series $x_t$ is said to be *weakly stationary* if both mean of $x_t$ and covariance between $x_t$ and $x_{t-l}$ are time-invariant 
$$E(x_t)=\mu$$ 
and for any $l\in(1,T)$
$$Cov(x_t, x_l)=\gamma_l$$

In other words, stationarity requires distribution of time series to be constant under time shift,
weak stationarity, which is assumed more often requires only fluctuation with constant variation
around constant level.

Let's start with a simple simulated time series:

```{r fig.width = 9.5, fig.height = 5.7}
x<-arima.sim(model = list(ar = c(0.75)), n = 2000)
y<-cumsum(x)

par(mfrow=c(1,2))
ts.plot(x,ylab='',main='Stationary')
ts.plot(y,ylab='', main='Non-Stationary')
```


**Why do we need to care about stationarity?**

Non-stationary series can strongly influence its behaviour and properties -- persistence of
shocks might be infinite.

*Spurious regressions:* 2 trending variables over time which are totally unrelated will have high $\mathbf{R^2}$

http://www.tylervigen.com/spurious-correlations

Assumptions for asymptotic analysis is not valid for non-stationary series *we can not test
hypotheses validly*

**Autocorrelation (ACF) plots**

Autocorrelation (ACF) plots are a useful visual tool in determining whether a series is stationary. ACF plots display correlation between a series and its lags. Useful in determining order of MA model.

In addition, Partial autocorrelation plots (PACF), display correlation between a variable and its lags that is not explained by previous lags. Useful in determining order of AR model.

```{r fig.width = 9.5, fig.height = 5.7}
par(mfrow=c(2,2))
acf(x, lag.max = 12, main="ACF: Stationary")
acf(y, lag.max = 12, main="ACF: Non-Stationary")

pacf(x, lag.max = 12, main="PACF")
pacf(y, lag.max = 12, main="PACF")

```

*Augmented Dickey-Fuller (ADF) test*

is a formal statistical test for stationarity. The null hypothesis assumes that the series is non-stationary. 

```{r}
adf.test(x, k = 1)
```

```{r}
adf.test(y, k = 1)
```

*What can I do about non-stationary data?*

Most ususal way is to first-difference

```{r fig.width = 9.5, fig.height = 5.7}
par(mfrow=c(1,2))
plot.ts(x)
plot.ts(diff(y,differences=1))

```

Are these identical? Do not forget that $y_t$ is just cummulated $x_t$ !!!
```{r}
y<-cumsum(x)
```



## Auto Regressive AR$(p)$ process

Let $\{ {x_t}\}$ be a covarinace stationary process, 
$$x_t = \mu + \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + ... + \alpha_p x_{t-p} + \epsilon_t$$,
whith $\epsilon_t$ being *iid* residual term. Let's simulate simple AR(1) with different parameters

```{r fig.width = 9.5, fig.height = 5.7}
y1 <- arima.sim(model = list(ar = c(0.25)), n = 600)
ts.plot(y1,ylab='')
```

Now play around with parameter $\phi$ to see how the process behaves
```{r fig.width = 9.5, fig.height = 5.7}
par(mfrow=c(1,2))
ts.plot(arima.sim(model = list(ar = c(0.9)), n = 600),ylab='')
ts.plot(arima.sim(model = list(ar = c(-0.9)), n = 600),ylab='')
```

ACF/PACF plots of AR process
```{r fig.width = 9.5, fig.height = 5.7}
par(mfrow=c(1,2))
acf(y1, lag.max = 12, main="ACF")
pacf(y1, lag.max = 12, main="PACF")

```

## Moving Average MA$(q)$ process

Let $\{ {x_t}\}$ be a covarinace stationary process, 
$$x_t = \mu + \epsilon_t + \beta_1 \epsilon_{t-1} + \beta_2 \epsilon_{t-2} + ... + \beta_1 \epsilon_{t-q}$$
whith $\epsilon_t$ being *iid* residual term. Let's simulate simple MA(1) with different parameters

```{r fig.width = 9.5, fig.height = 5.7}
y2 <- arima.sim(model = list(ma = c(0.25)), n = 600)
ts.plot(y2,ylab='')
```

Now play around with parameter $\phi$ to see how the process behaves
```{r fig.width = 9.5, fig.height = 5.7}
par(mfrow=c(1,2))
ts.plot(arima.sim(model = list(ma = c(0.9)), n = 600),ylab='')
ts.plot(arima.sim(model = list(ma = c(-0.9)), n = 600),ylab='')
```
ACF/PACF plots of MA process
```{r fig.width = 9.5, fig.height = 5.7}
par(mfrow=c(1,2))
acf(y2, lag.max = 12, main="ACF")
pacf(y2, lag.max = 12, main="PACF")

```


## ARIMA$(p,q)$
Let $\{ {x_t}\}$ be a covarinace stationary process, 
$$x_t = \mu + \alpha_1 x_{t-1} + ... + \alpha_p x_{t-p} + \epsilon_t + \beta_1 \epsilon_{t-1} + ... + \beta_1 \epsilon_{t-q}$$,
whith $\epsilon_t$ being *iid* residual term. Let's simulate simple ARIMA(2,2) with different parameters
```{r fig.width = 9.5, fig.height = 5.7}
y3 <- arima.sim(model = list(ar = c(0.25, -0.1),
                             ma = c(0.6, -0.15)), n = 600)
ts.plot(y3,ylab='')
```
Now play around with parameters!

ACF/PACF plots of ARMA process (bit more tricky right?)
```{r fig.width = 9.5, fig.height = 5.7}
par(mfrow=c(1,2))
acf(y3, lag.max = 12, main="ACF")
pacf(y3, lag.max = 12, main="PACF")

```

## Fitting ARIMA models: stationarity

Fitting can be tricky, consider following example, where we simulate AR(1) with coefficient 0.25.

ARMA(2,1) seems to be best fit, but look at coefficient!!!

```{r}
y0 <- arima.sim(model = list(ar = c(0.25)),n = 600)

arima(y0, order = c(1,0,0))
arima(y0, order = c(2,0,0))
arima(y0, order = c(2,0,1))

auto.arima(y0, seasonal=FALSE)

```

When is the model stationary????


##  Fitting ARIMA models: Empirical Example

Let's work with real data now, and estimate the ARIMA model.

First, always:

  * Simple plots, check data etc.
  * Check stationarity
  * Look at ACF/PACF
  * Assess order of AR/MA process

We will try to model and forecast volatility of stock market
```{r fig.width = 9.5, fig.height = 5.7}
# Load data
load(file="ExampleAR.RData")
plot.ts(RV)

# sp[lit data

RV0<-RV
RV<-RV[1:5135]

```

Stationary?
```{r}
adf.test(RV, k = 1)
```

Which model to choose?
```{r fig.width = 9.5, fig.height = 5.7}
par(mfrow=c(1,2))
acf(RV, lag.max = 12, main="ACF")
pacf(RV, lag.max = 12, main="PACF")
```

Let's start with simple AR(1) estimated via simple Least Squres
```{r fig.width = 9.5, fig.height = 5.7}
fit1<-lm(RV[2:5335] ~ 1 + RV[1:5334]) 
summary(fit1)

plot.ts(RV)
lines(fit1$fitted.values,col="red")
tsdisplay(residuals(fit1),lag.max=45,main='AR(1) residuals')
```
Impressive fit, even with very simple AR(1). But will it work? also residuals show that lot of dependence stayed in the data


Since it seems many lags are important, we can use convenient function to estimate more lags, we can also use *automatic* function to select lags based on information criteria
```{r fig.width = 9.5, fig.height = 5.7, cache = TRUE}
fit3<-auto.arima(RV, seasonal=FALSE)
summary(fit3)

plot.ts(RV)
lines(fit3$fitted,col="red")
tsdisplay(residuals(fit3),lag.max=45,main='Residuals')


fcast <- forecast(fit3, h=200)
plot(fcast)
```

### Fitting with Machine Learning based methods

What if I do not waht to select number of lags?

What if the dynamics is more complex, and instead of 
$$x_t = \mu + \alpha_1 x_{t-1} + ... + \alpha_p x_{t-p} + \epsilon_t + \beta_1 \epsilon_{t-1} + ... + \beta_1 \epsilon_{t-q}$$,
we should consider following
$$x_t = f(x_1,x_2,...,x_t)$$
The question is how to find $f(.)$?

**Neural Network Time Series Forecasts**

Feed-forward neural networks with a single hidden layer and lagged inputs for forecasting univariate time series.

Let's fit the Feed-Forward Neural Network
```{r fig.width = 9.5, fig.height = 5.7, cache = TRUE}
fitNN<-nnetar(RV,22,0,1,repeats=20)

fcastNN <- forecast(fitNN)
plot.ts(RV)
lines(fcastNN$fitted,col="red")

fcast2 <- forecast(fitNN, h=200)
plot(fcast2)
```

Compare Mean Squared Error
```{r}
cbind(c('AR(1)','ARIMA(4,1,3)','Neural Net'),
      c(100000*mean(na.omit(fit1$residuals^2)),
        100000*mean(na.omit(fit3$residuals^2)),
        100000*mean(na.omit(fcast2$residuals^2))))
```

```{r fig.width = 9.5, fig.height = 5.7}
plot(fcast2)
lines(fcast$mean,col="red")
lines(RV0,col="gray")
```

Can we improve further by "Let the machine choose" approach?
```{r fig.width = 9.5, fig.height = 5.7}
fitNN<-nnetar(RV)
fcastNN <- forecast(fitNN)
fcast3 <- forecast(fitNN, h=200)
plot(fcast3)
100000*mean(na.omit(fcast3$residuals^2))
```

